#!/usr/bin/env python

import datetime as dt
import os
import sys

import pydoop.hdfs as hdfs
# import pydoop.utils as utils

# Number of mappers. See also mapred.tasktracker.map.tasks.maximum in
# mapred-site.xml
MR_TASKS = 2
# PYDOOP_EXE = os.path.join(os.path.expanduser('~'), '.local', 'bin', 'pydoop')
PYDOOP_EXE = 'pydoop'


def timestamp_str():
    return dt.datetime.now().strftime('%Y%m%d-%H%M%S')


def main(local_in):
    ts = timestamp_str()
    in_dir = 'in-%s' % ts
    mr_in = 'mr-%s.in' % ts
    out_dir = 'out-%s' % ts
    mr_out = 'mr-%s.out' % ts

    print 'Local input dir:', local_in
    print 'HDFS input dir:', in_dir
    print 'HDFS output dir:', out_dir
    print 'MapReduce input file:', mr_in
    print 'MapReduce output file:', mr_out

    hdfs.cp(hdfs.path.join('file:', os.path.abspath(local_in)), in_dir)

    with hdfs.open(mr_in, 'w') as mrf:
        for f in hdfs.lsl(in_dir, recursive=True):
            if f['kind'] == 'file':
                mrf.write('%s\n' % f['name'])
    opts = '--num-reducers=0 -D mapred.map.tasks=%d -D out.dir=%s' % (
        MR_TASKS, out_dir)
    cmd = '%s script %s features.py %s %s' % (PYDOOP_EXE, opts, mr_in, mr_out)
    print 'Running %r' % (cmd,)
    os.system(cmd)

    hdfs.cp(out_dir, hdfs.path.join(
        'file:', os.path.abspath(os.path.basename(out_dir))))

if __name__ == '__main__':
    main(sys.argv[1])
